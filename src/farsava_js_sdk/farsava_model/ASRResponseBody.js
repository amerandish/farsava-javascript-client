/**
 * Farsava API
 * Farsava API. Speech Recognition and Text to Speech by applying powerful deep neural network models.
 *
 * OpenAPI spec version: 1.0.5
 * Contact: amir@amerandish.com
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 *
 */

import ApiClient from '../ApiClient';
import ASRStatus from './ASRStatus';
import SpeechRecognitionResult from './SpeechRecognitionResult';

/**
 * The ASRResponseBody model module.
 * @module farsava_js_sdk/farsava_model/ASRResponseBody
 * @version 1.0.5
 */
class ASRResponseBody {
    /**
     * Constructs a new <code>ASRResponseBody</code>.
     * @alias module:farsava_js_sdk/farsava_model/ASRResponseBody
     */
    constructor() { 
        
        ASRResponseBody.initialize(this);
    }

    /**
     * Initializes the fields of this object.
     * This method is used by the constructors of any subclasses, in order to implement multiple inheritance (mix-ins).
     * Only for internal use.
     */
    static initialize(obj) { 
    }

    /**
     * Constructs a <code>ASRResponseBody</code> from a plain JavaScript object, optionally creating a new instance.
     * Copies all relevant properties from <code>data</code> to <code>obj</code> if supplied or a new instance if not.
     * @param {Object} data The plain JavaScript object bearing properties of interest.
     * @param {module:farsava_js_sdk/farsava_model/ASRResponseBody} obj Optional instance to populate.
     * @return {module:farsava_js_sdk/farsava_model/ASRResponseBody} The populated <code>ASRResponseBody</code> instance.
     */
    static constructFromObject(data, obj) {
        if (data) {
            obj = obj || new ASRResponseBody();

            if (data.hasOwnProperty('transcriptionId')) {
                obj['transcriptionId'] = ApiClient.convertToType(data['transcriptionId'], 'String');
            }
            if (data.hasOwnProperty('duration')) {
                obj['duration'] = ApiClient.convertToType(data['duration'], 'Number');
            }
            if (data.hasOwnProperty('inferenceTime')) {
                obj['inferenceTime'] = ApiClient.convertToType(data['inferenceTime'], 'Number');
            }
            if (data.hasOwnProperty('status')) {
                obj['status'] = ASRStatus.constructFromObject(data['status']);
            }
            if (data.hasOwnProperty('results')) {
                obj['results'] = ApiClient.convertToType(data['results'], [SpeechRecognitionResult]);
            }
        }
        return obj;
    }

/**
     * Returns A UUID string specifying a unique pair of audio and recognitionResult. It can be used to retrieve this recognitionResult using transcription endpoint. asrLongRunning recognitionResult will only be available using transcription endpoint and this transcriptionId. 
     * @return {String}
     */
    getTranscriptionId() {
        return this.transcriptionId;
    }

    /**
     * Sets A UUID string specifying a unique pair of audio and recognitionResult. It can be used to retrieve this recognitionResult using transcription endpoint. asrLongRunning recognitionResult will only be available using transcription endpoint and this transcriptionId. 
     * @param {String} transcriptionId A UUID string specifying a unique pair of audio and recognitionResult. It can be used to retrieve this recognitionResult using transcription endpoint. asrLongRunning recognitionResult will only be available using transcription endpoint and this transcriptionId. 
     */
    setTranscriptionId(transcriptionId) {
        this['transcriptionId'] = transcriptionId;
    }
/**
     * Returns File duration in seconds.
     * @return {Number}
     */
    getDuration() {
        return this.duration;
    }

    /**
     * Sets File duration in seconds.
     * @param {Number} duration File duration in seconds.
     */
    setDuration(duration) {
        this['duration'] = duration;
    }
/**
     * Returns Total inference time in seconds.
     * @return {Number}
     */
    getInferenceTime() {
        return this.inferenceTime;
    }

    /**
     * Sets Total inference time in seconds.
     * @param {Number} inferenceTime Total inference time in seconds.
     */
    setInferenceTime(inferenceTime) {
        this['inferenceTime'] = inferenceTime;
    }
/**
     * @return {module:farsava_js_sdk/farsava_model/ASRStatus}
     */
    getStatus() {
        return this.status;
    }

    /**
     * @param {module:farsava_js_sdk/farsava_model/ASRStatus} status
     */
    setStatus(status) {
        this['status'] = status;
    }
/**
     * Returns Sequential list of transcription results corresponding to sequential portions of audio. May contain one or more recognition hypotheses (up to the maximum specified in maxAlternatives). These alternatives are ordered in terms of accuracy, with the top (first) alternative being the most probable, as ranked by the recognizer. 
     * @return {Array.<module:farsava_js_sdk/farsava_model/SpeechRecognitionResult>}
     */
    getResults() {
        return this.results;
    }

    /**
     * Sets Sequential list of transcription results corresponding to sequential portions of audio. May contain one or more recognition hypotheses (up to the maximum specified in maxAlternatives). These alternatives are ordered in terms of accuracy, with the top (first) alternative being the most probable, as ranked by the recognizer. 
     * @param {Array.<module:farsava_js_sdk/farsava_model/SpeechRecognitionResult>} results Sequential list of transcription results corresponding to sequential portions of audio. May contain one or more recognition hypotheses (up to the maximum specified in maxAlternatives). These alternatives are ordered in terms of accuracy, with the top (first) alternative being the most probable, as ranked by the recognizer. 
     */
    setResults(results) {
        this['results'] = results;
    }

}

/**
 * A UUID string specifying a unique pair of audio and recognitionResult. It can be used to retrieve this recognitionResult using transcription endpoint. asrLongRunning recognitionResult will only be available using transcription endpoint and this transcriptionId. 
 * @member {String} transcriptionId
 */
ASRResponseBody.prototype['transcriptionId'] = undefined;

/**
 * File duration in seconds.
 * @member {Number} duration
 */
ASRResponseBody.prototype['duration'] = undefined;

/**
 * Total inference time in seconds.
 * @member {Number} inferenceTime
 */
ASRResponseBody.prototype['inferenceTime'] = undefined;

/**
 * @member {module:farsava_js_sdk/farsava_model/ASRStatus} status
 */
ASRResponseBody.prototype['status'] = undefined;

/**
 * Sequential list of transcription results corresponding to sequential portions of audio. May contain one or more recognition hypotheses (up to the maximum specified in maxAlternatives). These alternatives are ordered in terms of accuracy, with the top (first) alternative being the most probable, as ranked by the recognizer. 
 * @member {Array.<module:farsava_js_sdk/farsava_model/SpeechRecognitionResult>} results
 */
ASRResponseBody.prototype['results'] = undefined;






export default ASRResponseBody;

